{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pbGanx_tVzjv"
      },
      "source": [
        "# Layout Parser - Model trained\n",
        "\n",
        "\n",
        "The model used here has been trained on BNF (Gallica)'s newspapers pages. they has been annotated and these annotations exported on COCO format.\n",
        "\n",
        "It can be used to extract the images the newspapers pages contained.\n",
        "\n",
        "The only thing to change is the paths to:\n",
        "- the model & config file (.pth & .yaml)\n",
        "- the pages you want to extract the pictures\n",
        "\n",
        "This notebook is made to be used linked to your Google Drive account. You can easily adapt it to use it with Jupyter.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RUKQXwAmUhmR",
        "outputId": "0e18fc2d-7466-40d6-edfc-3e130b0a6dc7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "12.1\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "print(torch.version.cuda)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VfbWFXAUH0l4",
        "outputId": "d1d0b323-bb42-4cd2-f82e-f7467a0eb077"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m47.5/47.5 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.2/42.2 kB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.5/79.5 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m117.0/117.0 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for detectron2 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for fvcore (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for iopath (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for antlr4-python3-runtime (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.2/19.2 MB\u001b[0m \u001b[31m37.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.9/57.9 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.6/5.6 MB\u001b[0m \u001b[31m66.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.8/2.8 MB\u001b[0m \u001b[31m60.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h2.3.0+cu121\n"
          ]
        }
      ],
      "source": [
        "# Install necessary packages\n",
        "# !pip install -q requests\n",
        "# !pip install -q --upgrade pip\n",
        "# !pip install -q opencv-python\n",
        "# !pip install -q torchvision\n",
        "# !pip install Pillow==9.5.0\n",
        "\n",
        "# Import necessary libraries\n",
        "import shutil\n",
        "import requests\n",
        "import glob\n",
        "import re\n",
        "import math\n",
        "import json\n",
        "import os\n",
        "import cv2\n",
        "from PIL import Image\n",
        "\n",
        "# Install and import layoutparser and detectron2\n",
        "!pip install -q 'git+https://github.com/facebookresearch/detectron2.git@v0.4#egg=detectron2'\n",
        "!pip install -q -U layoutparser\n",
        "\n",
        "import layoutparser as lp\n",
        "\n",
        "# Import PyTorch and empty the CUDA cache\n",
        "import torch\n",
        "torch.cuda.empty_cache()\n",
        "\n",
        "# Print the PyTorch version\n",
        "print(torch.__version__)\n",
        "\n",
        "import requests\n",
        "\n",
        "# Define the Hugging Face repository and model files\n",
        "repo_id = \"mgiardinetti/layout-parser-newspapers\"\n",
        "config_file = \"config.yaml\"\n",
        "model_file = \"model_final.pth\"\n",
        "\n",
        "# Create a directory to save the model\n",
        "save_directory = \"/content/layout-parser-newspapers\"\n",
        "os.makedirs(save_directory, exist_ok=True)\n",
        "\n",
        "# Download the configuration file\n",
        "config_url = f\"https://huggingface.co/{repo_id}/resolve/main/{config_file}\"\n",
        "config_path = os.path.join(save_directory, config_file)\n",
        "response = requests.get(config_url)\n",
        "with open(config_path, 'wb') as f:\n",
        "    f.write(response.content)\n",
        "\n",
        "# Download the model weights\n",
        "model_url = f\"https://huggingface.co/{repo_id}/resolve/main/{model_file}\"\n",
        "model_path = os.path.join(save_directory, model_file)\n",
        "response = requests.get(model_url)\n",
        "with open(model_path, 'wb') as f:\n",
        "    f.write(response.content)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bCzbQ6TI-DtI",
        "outputId": "bcf9d705-1c69-43a0-e6e8-f197a7d7c711"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Oa3ln2dUWyzb",
        "outputId": "0badf93e-07fa-4d45-f8ce-7dc927913bd0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Modèle importé\n"
          ]
        }
      ],
      "source": [
        "import layoutparser as lp\n",
        "\n",
        "# Model and configuration file paths\n",
        "model = \"/content/layout-parser-newspapers/model_final.pth\"\n",
        "config = \"/content/layout-parser-newspapers/config.yaml\"\n",
        "\n",
        "# Association id:nom for each class to detect\n",
        "labels = \"0:illustration 1:legende\"\n",
        "\n",
        "# Parse labels\n",
        "dictlabels = {}\n",
        "if labels:\n",
        "    for asso in labels.split(\" \"):\n",
        "        key, value = asso.split(\":\")\n",
        "        dictlabels[int(key)] = value\n",
        "\n",
        "# Load the model\n",
        "try:\n",
        "    if dictlabels:\n",
        "        model = lp.models.Detectron2LayoutModel(\n",
        "            config_path=config,\n",
        "            model_path=model,\n",
        "            label_map=dictlabels\n",
        "        )\n",
        "    else:\n",
        "        model = lp.models.Detectron2LayoutModel(\n",
        "            config_path=config,\n",
        "            model_path=model\n",
        "        )\n",
        "    print(\"Modèle importé\")\n",
        "except Exception as e:\n",
        "    print(f\"Le modèle n'a pas été importé: {e}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "m8HQQNJUQebE"
      },
      "outputs": [],
      "source": [
        "import cv2\n",
        "\n",
        "def resize_image(img, shape):\n",
        "    \"\"\" Resize image to a specific shape, maintaining aspect ratio. \"\"\"\n",
        "    h, w = img.shape[:2]\n",
        "    aspect_ratio = h / w\n",
        "    target_h, target_w = shape\n",
        "    if aspect_ratio > 1:\n",
        "        new_h, new_w = target_h, int(target_h / aspect_ratio)\n",
        "    else:\n",
        "        new_w, new_h = target_w, int(target_w * aspect_ratio)\n",
        "\n",
        "    resized_img = cv2.resize(img, (new_w, new_h), interpolation=cv2.INTER_AREA)\n",
        "    return resized_img\n",
        "\n",
        "def add_borders(img, shape):\n",
        "    \"\"\" Add black borders to maintain the desired aspect ratio. \"\"\"\n",
        "    h, w = img.shape[:2]\n",
        "    top = bottom = (shape[0] - h) // 2\n",
        "    left = right = (shape[1] - w) // 2\n",
        "    color = [0, 0, 0]\n",
        "    new_img = cv2.copyMakeBorder(img, top, bottom, left, right, cv2.BORDER_CONSTANT, value=color)\n",
        "    return new_img"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import glob\n",
        "import re\n",
        "import cv2\n",
        "from PIL import Image, UnidentifiedImageError\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Define directories\n",
        "extractions_path = '/content/drive/My Drive/similaires_rol_extracted/'\n",
        "pages_directory = '/content/drive/My Drive/similaires_rol/'\n",
        "\n",
        "seuil_score = 0.9\n",
        "\n",
        "all_pages_extracted = glob.glob(os.path.join(extractions_path, '*.jpg'))\n",
        "all_pages = glob.glob(os.path.join(pages_directory, '*.jpg'))\n",
        "\n",
        "print(len(all_pages_extracted))\n",
        "print(len(all_pages))\n",
        "\n",
        "all_pages_filtered = []\n",
        "for page in all_pages:\n",
        "  filtered = False\n",
        "  id = page.split('/')[-1].split('.')[0]\n",
        "  for page_extracted in all_pages_extracted:\n",
        "    id_extracted = page_extracted.split('/')[-1].split('.')[0]\n",
        "    if id in id_extracted:\n",
        "      filtered = True\n",
        "    if filtered:\n",
        "      break\n",
        "  if not filtered:\n",
        "     all_pages_filtered.append(page)\n",
        "print(len(all_pages_filtered))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-dGNkhdnhMG2",
        "outputId": "9242e1bf-7559-4aa0-ac1b-03a537c84a69"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "899\n",
            "2015\n",
            "1745\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bVQ_ByQmx9-k",
        "outputId": "748c50b4-daa8-4025-ff7d-5509f2d80ded"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|          | 0/1745 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/torch/functional.py:512: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at ../aten/src/ATen/native/TensorShape.cpp:3587.)\n",
            "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n",
            "  0%|          | 1/1745 [00:27<13:09:53, 27.17s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "_________________Extraction of 0 pictures_________________\n"
          ]
        }
      ],
      "source": [
        "for page_path in tqdm(all_pages_filtered):\n",
        "    page_name = os.path.basename(page_path)\n",
        "\n",
        "    # Read and convert image\n",
        "    try :\n",
        "      image = cv2.imread(page_path)\n",
        "      image = image[..., ::-1]\n",
        "\n",
        "      # Detect layout\n",
        "      layout = model.detect(image)\n",
        "      layout = [x for x in layout if x.score > seuil_score]\n",
        "      print(f\"_________________Extraction of {len(layout)} pictures_________________\")\n",
        "\n",
        "      # Extract bounding box coordinates\n",
        "      layout_str = str(layout)\n",
        "      x1 = list(map(int, re.findall(r'x_1=(\\d+)', layout_str)))\n",
        "      y1 = list(map(int, re.findall(r'y_1=(\\d+)', layout_str)))\n",
        "      x2 = list(map(int, re.findall(r'x_2=(\\d+)', layout_str)))\n",
        "      y2 = list(map(int, re.findall(r'y_2=(\\d+)', layout_str)))\n",
        "      # Crop and save images\n",
        "      for i, (x1_i, y1_i, x2_i, y2_i, lay) in enumerate(zip(x1, y1, x2, y2,layout)):\n",
        "          try :\n",
        "              img = Image.open(page_path)\n",
        "              cropped = img.crop((x1_i, y1_i, x2_i, y2_i))\n",
        "              cropped = np.array(cropped)\n",
        "              cropped_file_name = f\"{os.path.splitext(page_name)[0]}_{i+1:02d}_{np.round(lay.score,3)}.jpg\"\n",
        "              cropped = resize_image(cropped, (1024,1024))\n",
        "              cropped = add_borders(cropped, (1024,1024))\n",
        "              cv2.imwrite(os.path.join(extractions_path, cropped_file_name), cropped)\n",
        "          except Exception as error:\n",
        "              print(error)\n",
        "              print(\"Error in subimage\")\n",
        "              continue\n",
        "    except Exception as error:\n",
        "        print(error)\n",
        "        print(\"Error reading image\")\n",
        "        continue"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.10"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}